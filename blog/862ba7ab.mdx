---
authors:
- PuQing
date: 2023-12-05 18:56
keywords:
- 参数估计
- 概率论
- 混合模型
tags:
- 参数估计
- 概率论
- 混合模型
---
# EM 算法 - 收敛性

`Expectation Maximization(EM)` 算法，该算法用于解决具有隐变量的混合模型的高斯和分布（例子可以见 [三硬币模型](./04c6f12c#%E4%B8%89%E7%A1%AC%E5%B8%81%E6%A8%A1%E5%9E%8B)）。在比较理想的情况中，我们可以直接得出我们求得的参数的解析解，比如：
$\mathrm{MLE}:p(X\mid \theta)$。我们想要求解的结果就是：

$$
\theta_{MLE} = \arg \max_{\theta}\sum_{i=1}^N \log p(x_{i}\mid \theta)
$$
其中，$\sum_{i=1}^N\log p(x_{i}\mid \theta)$ 也被我们称为 [对数似然函数](./967aee09#%5E67c3dc)。但是一旦引入隐变量，似然函数变为：


<!--truncate-->
$$
\mathcal{L}(\theta) = \sum_{i=1}^N \log p(x_{i}\mid \theta)=\sum_{i}^N \log \left[ \sum_{z_{i}} p(x_{i},z_{i}\mid \theta) \right] 
$$
:::tip 其他格式：

$$
\mathcal{L}(\theta) = \sum_{i}^N \log \left[ \sum_{z_{i}}p(x_{i},z_{i};\theta) \right] 
$$
写成条件概率的形式 (该种写法在 [混合模型](./04c6f12c#%5Ed1001b) 中有过推导)

$$
\mathcal{L}(\theta) = \sum_{i}^N \log \left[ \sum_{z_{i}}p(x_{i}\mid z_{i};\theta)p(z_{i}\mid \theta) \right]
$$
:::
这个式子变的难以求解

:::example
就拿上面 [三硬币模型](./04c6f12c#%E4%B8%89%E7%A1%AC%E5%B8%81%E6%A8%A1%E5%9E%8B) 来说，其似然函数可写为，

$$
P(Y \mid \theta)=\prod_{j=1}^{n}\left[\pi p^{y_{j}}(1-p)^{1-y_{j}}+(1-\pi) q^{y_{j}}(1-q)^{1-y_{j}}\right]
$$
所以似然估计为

$$
\begin{equation}
\hat{\theta}=\arg \max _{\theta} \log P(Y \mid \theta)
\end{equation}
$$
即求下面函数的零点：

$$
\sum_{j=1}^{n}\log\left(\pi p^{y_{j}}(1-p)^{1-y_{j}}+(1-\pi) q^{y_{j}}(1-q)^{1-y_{j}}\right)
$$
分别对 $\pi,p,q$ 求偏导：

$$
\begin{aligned}
\frac{\partial \log P(Y\mid \theta)}{\partial\pi}&= \sum_{j=1}^n\frac{p^{y_{i}}(1-p)^{1-y_{i}}-q^{y_{i}}(1-q)^{1-y_{i}}}{p^{y_{i}} \pi(1-p)^{1-y_{i}}-q^{y_{i}}(1-q)^{1-y_{i}}(\pi-1)}\\
\frac{\partial \log P(Y\mid \theta)}{\partial p}&= \sum_{j=1}^n\frac{\pi\left(-p^{y_{i}} y_{i}(1-p)^{2-y_{i}}+p^{y_{i}+1}(1-p)^{1-y_{i}}\left(1-y_{i}\right)\right)}{p(p-1)\left(p^{y_{i}} \pi(1-p)^{1-y_{i}}-q^{y_{i}}(1-q)^{1-y_{i}}(\pi-1)\right)} \\
\frac{\partial \log P(Y\mid \theta)}{\partial q}&=\sum_{j=1}^n\frac{(\pi-1)\left(q^{y_{i}} y_{i}(1-q)^{2-y_{i}}+q^{y_{i}+1}(1-q)^{1-y_{i}}\left(y_{i}-1\right)\right)}{q(q-1)\left(p^{y_{i}} \pi(1-p)^{1-y_{i}}-q^{y_{i}}(1-q)^{1-y_{i}}(\pi-1)\right)}
\end{aligned}
$$
…这是及其难以求解的

:::
## EM 算法概述

期望最大算法整体上利用了 [MM algorithm](./7f530326) 核心思想

:::warning
在 [MM algorithm](./7f530326) 中我们要求函数的最大值/最小值，我们可以构造一个代理函数 $Q(\theta,\theta_{t})$，并且这个代理函数 $Q(\theta,\theta_{t})$ 在 EM 算法中是对数似然函数 $\mathcal{L}(\theta)$ 的紧下界，应该满足如下两个条件：

$$
\begin{split}
Q(\theta,\theta_{t}) \leq \mathcal{L}(\theta)\\
Q(\theta_{t},\theta_{t}) = \mathcal{L}(\theta_{t})
\end{split}
$$
这样的话我们的目标由最大化对数似然函数 $\mathcal{L}(\theta)$ 转变为了一步一步迭代紧下界 $Q(\theta,\theta_{t})$，于是：

$$
\theta_{t+1} = \arg \max_{\theta}Q(\theta,\theta_{t})
$$
因此，利用 [MM algorithm](./7f530326) 算法从第 $t$ 步迭代到第 $t+1$ 步利用了如下不等式关系

$$
\mathcal{L}(\theta_{t+1}) \geq Q(\theta_{t+1},\theta_{t}) \geq Q(\theta_{t},\theta_{t}) = \mathcal{L}(\theta_{t})
$$
:::
## 相关资料

- [清晰理解期望最大(EM)算法——从边界优化（band optimization）的角度谈起 - 知乎](https://zhuanlan.zhihu.com/p/462102937#:~:text=%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7%EF%BC%88EM%EF%BC%89%E7%AE%97%E6%B3%95%E6%95%B4%E4%BD%93%E5%88%A9%E7%94%A8%E4%BA%86%E8%BE%B9%E7%95%8C%E4%BC%98%E5%8C%96%EF%BC%88band%20optimization%EF%BC%89%E7%9A%84%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3%EF%BC%8C%E5%9B%A0%E6%AD%A4%E4%B8%8B%E6%96%87%E9%A6%96%E5%85%88%E5%AF%B9%E8%BE%B9%E7%95%8C%E4%BC%98%E5%8C%96%E8%BF%9B%E8%A1%8C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D%E3%80%82%201.%E8%BE%B9%E7%95%8C%E4%BC%98%E5%8C%96%EF%BC%88band%20optimization%EF%BC%89%E7%AE%80%E4%BB%8B%EF%BC%9A%20%E5%81%87%E8%AE%BE%E6%88%91%E4%BB%AC%E7%9A%84%E7%9B%AE%E6%A0%87%E6%98%AF%E9%9C%80%E8%A6%81%E6%9C%80%E5%A4%A7%E5%8C%96%E5%AF%B9%E6%95%B0%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0%20LL%20%28theta%29%20%EF%BC%8C%E5%85%B6%E4%B8%AD,%E7%9A%84%E7%B4%A7%E4%B8%8B%E7%95%8C%EF%BC%88tight%20lowerbound%EF%BC%89%E3%80%82%20Q%20%28theta%2Ctheta_t%29%20%E8%8B%A5%E8%A6%81%E7%A7%B0%E4%B8%BA%20LL%20%28theta%29%20%E7%9A%84%E7%B4%A7%E4%B8%8B%E7%95%8C%EF%BC%8C%E5%BA%94%E8%AF%A5%E6%BB%A1%E8%B6%B3%E5%A6%82%E4%B8%8B%E4%B8%A4%E4%B8%AA%E6%9D%A1%E4%BB%B6%EF%BC%9A)
